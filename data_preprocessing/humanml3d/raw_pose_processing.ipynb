{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from scipy.spatial.transform import Rotation\n",
    "import pytorch3d\n",
    "\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/media/erik/DATA'\n",
    "# raw_data_dir = f'{data_root}/behave_raw'\n",
    "# stage_1_output = f'{data_root}/behave_processed/pose_data_behave'\n",
    "# stage_2_output = f'{data_root}/behave_processed/joints_behave'\n",
    "\n",
    "raw_data_dir = f'{data_root}/grab/grab_preprocessed'\n",
    "stage_1_output = f'{data_root}/grab/pose_data_grab'\n",
    "stage_2_output = f'{data_root}/grab/joints_grab'\n",
    "\n",
    "# raw_data_dir = '/media/erik/DATA/grab_preprocessed'\n",
    "# stage_1_output = '/media/erik/DATA/pose_data_grab'\n",
    "# stage_2_output = '/media/erik/DATA/joints_grab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation device: cpu\n"
     ]
    }
   ],
   "source": [
    "# comp_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# this needs some really high memory GPUs, so default to CPU\n",
    "comp_device = torch.device(\"cpu\")\n",
    "\n",
    "print('Computation device:', comp_device)\n",
    "# %%\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "male_bm_path = './body_models/smplh/male/model.npz'\n",
    "male_dmpl_path = './body_models/dmpls/male/model.npz'\n",
    "\n",
    "female_bm_path = './body_models/smplh/female/model.npz'\n",
    "female_dmpl_path = './body_models/dmpls/female/model.npz'\n",
    "\n",
    "num_betas = 10 # number of body parameters\n",
    "num_dmpls = 8 # number of DMPL parameters\n",
    "\n",
    "# male_bm = BodyModel(bm_fname=male_bm_path, num_betas=num_betas, num_dmpls=num_dmpls, dmpl_fname=male_dmpl_path).to(comp_device)\n",
    "male_bm = BodyModel(bm_fname=male_bm_path, num_betas=num_betas).to(comp_device)\n",
    "faces = c2c(male_bm.f)\n",
    "\n",
    "# female_bm = BodyModel(bm_fname=female_bm_path, num_betas=num_betas, num_dmpls=num_dmpls, dmpl_fname=female_dmpl_path).to(comp_device)\n",
    "female_bm = BodyModel(bm_fname=female_bm_path, num_betas=num_betas).to(comp_device)\n",
    "\n",
    "paths = []\n",
    "folders = []\n",
    "dataset_names = []\n",
    "for root, dirs, files in os.walk(raw_data_dir):\n",
    "    folders.append(root)\n",
    "    for name in files:\n",
    "        dataset_name = root.split('/')[1]\n",
    "        if dataset_name not in dataset_names:\n",
    "            dataset_names.append(dataset_name)\n",
    "        if 'smpl_fit_all.npz' in files:\n",
    "            paths.append(os.path.join(root, 'smpl_fit_all.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d.transforms\n",
    "\n",
    "\n",
    "save_folders = [folder.replace(raw_data_dir, stage_1_output) for folder in folders]\n",
    "for folder in save_folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "group_path = [[path for path in paths if name in path] for name in dataset_names]\n",
    "\n",
    "# %%\n",
    "# trans_matrix = np.array([[1.0, 0.0, 0.0],\n",
    "#                             [0.0, -1.0, 0.0],\n",
    "#                             [0.0, 0.0, -1.0]])\n",
    "\n",
    "trans_matrix = np.array([[1.0, 0.0, 0.0],\n",
    "                            [0.0, 1.0, 0.0],\n",
    "                            [0.0, 0.0, 1.0]])\n",
    "\n",
    "ex_fps = 30\n",
    "def amass_to_pose(src_path, save_path):\n",
    "    seq_info_path = src_path.replace(\"smpl_fit_all.npz\", \"info.json\")\n",
    "    with open(seq_info_path, \"r\") as f:\n",
    "        seq_info = json.load(f)\n",
    "    gender = seq_info[\"gender\"]\n",
    "\n",
    "    src_path_obj = src_path.replace('smpl_fit_all.npz', 'object_fit_all.npz')\n",
    "\n",
    "    bdata = np.load(src_path, allow_pickle=True)\n",
    "    bdata_obj = np.load(src_path_obj, allow_pickle=True)\n",
    "\n",
    "    # load hand data\n",
    "    src_path_lhand = src_path.replace('smpl_fit_all.npz', 'lhand_data.npz')\n",
    "    src_path_rhand = src_path.replace('smpl_fit_all.npz', 'rhand_data.npz')\n",
    "    data_lhand = np.load(src_path_lhand, allow_pickle=True)\n",
    "    data_rhand = np.load(src_path_rhand, allow_pickle=True)\n",
    "    # bdata_lhand = bdata_lhand['joints'] # 1 x T x 21 x 3\n",
    "    # bdata_rhand = bdata_rhand['joints'] # 1 x T x 21 x 3\n",
    "\n",
    "    # bdata_lhand = torch.Tensor(bdata_lhand).to(comp_device).squeeze(0) # T x 21 x 3\n",
    "    # bdata_rhand = torch.Tensor(bdata_rhand).to(comp_device).squeeze(0) # T x 21 x 3\n",
    "\n",
    "    bdata_lhand = data_lhand['hand_pose'].squeeze(0) # T x 24\n",
    "    bdata_rhand = data_rhand['hand_pose'].squeeze(0) # T x 24\n",
    "\n",
    "    # print(f'hand data shape: {bdata_lhand.shape}')\n",
    "\n",
    "    rot_axisang_lhand = np.array(data_lhand['global_orient'])\n",
    "    rot_axisang_rhand = np.array(data_rhand['global_orient'])\n",
    "\n",
    "    # TODO: fix this, the rotation is AXIS ANGLE, not euler angles, need to convert this approapriately on both ends of pipeline\n",
    "\n",
    "    # convert to 6d representation\n",
    "    # rot_lhand = Rotation.from_euler('xyz', rot_euler_lhand).as_matrix()\n",
    "    # rot_rhand = Rotation.from_euler('xyz', rot_euler_rhand).as_matrix()\n",
    "\n",
    "    rot_lhand = pytorch3d.transforms.axis_angle_to_matrix(torch.Tensor(rot_axisang_lhand).to(comp_device))\n",
    "    rot_rhand = pytorch3d.transforms.axis_angle_to_matrix(torch.Tensor(rot_axisang_rhand).to(comp_device))\n",
    "\n",
    "    # convert to continuous 6d representation\n",
    "    rot_lhand = pytorch3d.transforms.matrix_to_rotation_6d(rot_lhand).squeeze(0)\n",
    "    rot_rhand = pytorch3d.transforms.matrix_to_rotation_6d(rot_rhand).squeeze(0)\n",
    "\n",
    "    bdata_lhand = np.concatenate([rot_lhand, bdata_lhand], axis=-1) # T x 30\n",
    "    bdata_rhand = np.concatenate([rot_rhand, bdata_rhand], axis=-1) # T x 30\n",
    "    \n",
    "    assert bdata_lhand.shape[1] == 30 and bdata_rhand.shape[1] == 30\n",
    "\n",
    "\n",
    "    fps = 120\n",
    "    frame_number = bdata['trans'].shape[0]\n",
    "\n",
    "    fId = 0 # frame id of the mocap sequence\n",
    "    pose_seq = []\n",
    "    if gender == 'male':\n",
    "        bm = male_bm\n",
    "    else:\n",
    "        bm = female_bm\n",
    "    down_sample = int(fps / ex_fps)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        root_orient = torch.Tensor(bdata['poses'][::down_sample, :3]).to(comp_device) # controls the global root orientation \n",
    "\n",
    "        pose_body = torch.Tensor(bdata['poses'][::down_sample, 3:66]).to(comp_device) # controls the body\n",
    "        pose_hand = torch.Tensor(bdata['poses'][::down_sample, 66:]).to(comp_device) # controls the finger articulation\n",
    "        betas = torch.Tensor(bdata['betas'][::down_sample]).to(comp_device) # controls the body shape\n",
    "        trans = torch.Tensor(bdata['trans'][::down_sample]).to(comp_device)    \n",
    "        body = bm(pose_body=pose_body, pose_hand=pose_hand, betas=betas, root_orient=root_orient)\n",
    "        joint_loc = body.Jtr + trans[:, None]\n",
    "        pose_seq = joint_loc\n",
    "\n",
    "    # pose_seq = torch.cat(pose_seq, dim=0)\n",
    "    \n",
    "    pose_seq_np = pose_seq.detach().cpu().numpy()\n",
    "    pose_seq_np_n = np.dot(pose_seq_np, trans_matrix)\n",
    "    np.save(save_path, pose_seq_np_n)\n",
    "\n",
    "    # process obj pose data\n",
    "    angle, trans = bdata_obj['angles'], bdata_obj['trans']\n",
    "    rot = Rotation.from_rotvec(angle).as_matrix()\n",
    "    mat = np.eye(4)[np.newaxis].repeat(rot.shape[0], axis=0)\n",
    "    mat[:, :3, :3] = rot\n",
    "    mat[:, :3, 3] = trans\n",
    "    trans_matrix_eye4 = np.eye(4)[np.newaxis]\n",
    "    trans_matrix_eye4[0, :3, :3] = trans_matrix\n",
    "    mat = trans_matrix_eye4 @ mat\n",
    "\n",
    "    rot, trans = mat[:, :3, :3], mat[:, :3, 3]\n",
    "    rot = Rotation.from_matrix(rot).as_rotvec()\n",
    "    obj_pose = np.concatenate([rot, trans], axis=-1)\n",
    "\n",
    "    # downsample obj pose\n",
    "    obj_pose = obj_pose[::down_sample]\n",
    "    bdata_lhand = bdata_lhand[::down_sample]\n",
    "    bdata_rhand = bdata_rhand[::down_sample]\n",
    "\n",
    "    save_path_obj = save_path.replace('smpl_fit_all.npy', 'object_fit_all.npy')\n",
    "    np.save(save_path_obj, obj_pose)\n",
    "\n",
    "    save_path_lhand = save_path.replace('smpl_fit_all.npy', 'lhand_data.npy')\n",
    "    np.save(save_path_lhand, bdata_lhand)\n",
    "\n",
    "    save_path_rhand = save_path.replace('smpl_fit_all.npy', 'rhand_data.npy')\n",
    "    np.save(save_path_rhand, bdata_rhand)\n",
    "    \n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 1/9345 [00:00<22:48,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (960, 24)\n",
      "hand data shape: (960, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 3/9345 [00:00<15:20, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (960, 24)\n",
      "hand data shape: (960, 24)\n",
      "hand data shape: (960, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 7/9345 [00:00<13:21, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (960, 24)\n",
      "hand data shape: (960, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 9/9345 [00:00<16:26,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (1026, 24)\n",
      "hand data shape: (1026, 24)\n",
      "hand data shape: (1026, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 11/9345 [00:01<15:11, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (1026, 24)\n",
      "hand data shape: (1026, 24)\n",
      "hand data shape: (1026, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 13/9345 [00:01<14:31, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (1026, 24)\n",
      "hand data shape: (2268, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 16/9345 [00:01<21:23,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (2268, 24)\n",
      "hand data shape: (2268, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 18/9345 [00:02<24:29,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (2268, 24)\n",
      "hand data shape: (2268, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 20/9345 [00:02<25:38,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (2268, 24)\n",
      "hand data shape: (2268, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 22/9345 [00:02<29:06,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (1471, 24)\n",
      "hand data shape: (1471, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 24/9345 [00:03<23:52,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (1471, 24)\n",
      "hand data shape: (1471, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 26/9345 [00:03<21:17,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand data shape: (1471, 24)\n",
      "hand data shape: (1471, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: erik:   0%|          | 26/9345 [00:03<21:06,  7.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mreplace(raw_data_dir, stage_1_output)\n\u001b[1;32m     13\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m save_path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnpy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     fps \u001b[38;5;241m=\u001b[39m \u001b[43mamass_to_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m cur_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(paths)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed / All (fps \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m (fps, cur_count, all_count) )\n",
      "Cell \u001b[0;32mIn[29], line 86\u001b[0m, in \u001b[0;36mamass_to_pose\u001b[0;34m(src_path, save_path)\u001b[0m\n\u001b[1;32m     84\u001b[0m betas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(bdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m][::down_sample])\u001b[38;5;241m.\u001b[39mto(comp_device) \u001b[38;5;66;03m# controls the body shape\u001b[39;00m\n\u001b[1;32m     85\u001b[0m trans \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(bdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m'\u001b[39m][::down_sample])\u001b[38;5;241m.\u001b[39mto(comp_device)    \n\u001b[0;32m---> 86\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[43mbm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_hand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_hand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_orient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_orient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m joint_loc \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mJtr \u001b[38;5;241m+\u001b[39m trans[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     88\u001b[0m pose_seq \u001b[38;5;241m=\u001b[39m joint_loc\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ethz/digital-humans/dex-hoi/data_preprocessing/humanml3d/human_body_prior/body_model/body_model.py:241\u001b[0m, in \u001b[0;36mBodyModel.forward\u001b[0;34m(self, root_orient, pose_body, pose_hand, pose_jaw, pose_eye, betas, trans, dmpls, expression, v_template, joints, v_shaped, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m     shape_components \u001b[38;5;241m=\u001b[39m betas\n\u001b[1;32m    239\u001b[0m     shapedirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshapedirs\n\u001b[0;32m--> 241\u001b[0m verts, Jtr \u001b[38;5;241m=\u001b[39m \u001b[43mlbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_pose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mshapedirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshapedirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposedirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposedirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mJ_regressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJ_regressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkintree_table\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlbs_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_shaped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv_shaped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m Jtr \u001b[38;5;241m=\u001b[39m Jtr \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    248\u001b[0m verts \u001b[38;5;241m=\u001b[39m verts \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/ethz/digital-humans/dex-hoi/data_preprocessing/humanml3d/human_body_prior/body_model/lbs.py:244\u001b[0m, in \u001b[0;36mlbs\u001b[0;34m(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents, lbs_weights, joints, pose2rot, v_shaped, dtype)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# (N x V x (J + 1)) x (N x (J + 1) x 16)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m num_joints \u001b[38;5;241m=\u001b[39m J_regressor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 244\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_joints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    247\u001b[0m homogen_coord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones([batch_size, v_posed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    248\u001b[0m                            dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    249\u001b[0m v_posed_homo \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v_posed, homogen_coord], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "group_path = group_path\n",
    "all_count = sum([len(paths) for paths in group_path])\n",
    "cur_count = 0\n",
    "\n",
    "import time\n",
    "for paths in group_path:\n",
    "    dataset_name = paths[0].split('/')[2]\n",
    "    pbar = tqdm(paths)\n",
    "    pbar.set_description('Processing: %s'%dataset_name)\n",
    "    fps = 0\n",
    "    for path in pbar:\n",
    "        save_path = path.replace(raw_data_dir, stage_1_output)\n",
    "        save_path = save_path[:-3] + 'npy'\n",
    "        fps = amass_to_pose(path, save_path)\n",
    "        \n",
    "    cur_count += len(paths)\n",
    "    print('Processed / All (fps %d): %d/%d'% (fps, cur_count, all_count) )\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m         data[:, left_hand_chain] \u001b[38;5;241m=\u001b[39m tmp\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m---> 24\u001b[0m total_amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mgroup_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     25\u001b[0m fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(total_amount)):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import codecs as cs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import join as pjoin\n",
    "\n",
    "def swap_left_right(data):\n",
    "    assert len(data.shape) == 3 and data.shape[-1] == 3\n",
    "    data = data.copy()\n",
    "    data[..., 0] *= -1\n",
    "    right_chain = [2, 5, 8, 11, 14, 17, 19, 21]\n",
    "    left_chain = [1, 4, 7, 10, 13, 16, 18, 20]\n",
    "    left_hand_chain = [22, 23, 24, 34, 35, 36, 25, 26, 27, 31, 32, 33, 28, 29, 30]\n",
    "    right_hand_chain = [43, 44, 45, 46, 47, 48, 40, 41, 42, 37, 38, 39, 49, 50, 51]\n",
    "    tmp = data[:, right_chain]\n",
    "    data[:, right_chain] = data[:, left_chain]\n",
    "    data[:, left_chain] = tmp\n",
    "    if data.shape[1] > 24:\n",
    "        tmp = data[:, right_hand_chain]\n",
    "        data[:, right_hand_chain] = data[:, left_hand_chain]\n",
    "        data[:, left_hand_chain] = tmp\n",
    "    return data\n",
    "\n",
    "total_amount = len(group_path[0])\n",
    "fps = 30\n",
    "\n",
    "for i in tqdm(range(total_amount)):\n",
    "    path = group_path[0][i]\n",
    "    source_path = path.replace(raw_data_dir, stage_1_output)\n",
    "    source_path = source_path[:-3] + 'npy'\n",
    "    try:\n",
    "        data = np.load(source_path)\n",
    "    except:\n",
    "        print('Error: ', source_path)\n",
    "        continue\n",
    "    new_name = source_path.split('/')[-2]\n",
    "    # data[..., 0] *= -1\n",
    "    \n",
    "    # data_m = swap_left_right(data)\n",
    "\n",
    "    source_path_obj = source_path.replace('smpl_fit_all.npy', 'object_fit_all.npy')\n",
    "    data_obj = np.load(source_path_obj)\n",
    "    if not os.path.exists(stage_2_output):\n",
    "        os.makedirs(stage_2_output, exist_ok=True)\n",
    "\n",
    "    np.save(pjoin(stage_2_output, new_name), data)\n",
    "    # np.save(pjoin(save_dir, 'M'+new_name), data_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
