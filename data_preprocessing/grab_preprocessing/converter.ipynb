{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import os, glob\n",
    "import smplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GRAB.tools.objectmodel import ObjectModel\n",
    "from GRAB.tools.utils import parse_npz, prepare_params, params2torch, to_cpu, append2dict\n",
    "from GRAB.tools.meshviewer import Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data directory: /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab\n",
      "AMASS directory: /media/erik/DATA/grab_amass\n",
      "Output directory: /media/erik/DATA/grab_preprocessed\n",
      "SMPLX/MANO Model directory: /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/body_models\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "grab_dir = os.path.abspath('./data/grab_unzipped/grab')\n",
    "grab_amass_dir = os.path.abspath('/media/erik/DATA/grab_amass')\n",
    "out_dir = os.path.abspath('/media/erik/DATA/grab_preprocessed')\n",
    "model_path = os.path.abspath('./body_models/')\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "print('Using device:', device)\n",
    "print('Data directory:', grab_dir)\n",
    "print('AMASS directory:', grab_amass_dir)\n",
    "print('Output directory:', out_dir)\n",
    "print('SMPLX/MANO Model directory:', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s5 with 106 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s4 with 113 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s1 with 198 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s9 with 125 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s3 with 125 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s7 with 120 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s8 with 162 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s10 with 145 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s2 with 93 files, creating output folder structure...\n",
      "Adding directory /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s6 with 148 files, creating output folder structure...\n",
      "No corresponding task for banana_eat_1 in s5 found\n",
      "No corresponding task for phone_lift in s5 found\n",
      "No corresponding task for cylindersmall_lift in s5 found\n",
      "No corresponding task for airplane_fly_1 in s5 found\n",
      "No corresponding task for multi_bottle_whineglass_1 in s3 found\n"
     ]
    }
   ],
   "source": [
    "# map directory names to lists of files in them\n",
    "data_dict = {}\n",
    "\n",
    "for root, dirs, files in os.walk(grab_dir):\n",
    "    if len(files) > 0:\n",
    "        print(f'Adding directory {root} with {len(files)} files, creating output folder structure...')\n",
    "        subject_id = os.path.basename(root)\n",
    "        data_dict[subject_id] = {}\n",
    "\n",
    "        for file in files:\n",
    "            task_description = os.path.splitext(file)[0]\n",
    "            file_in = os.path.join(root, file)\n",
    "            file_out_dir = os.path.join(out_dir, f'{subject_id}_{task_description}')\n",
    "            os.makedirs(file_out_dir, exist_ok=True)\n",
    "            data_dict[subject_id][task_description] = {'full_info': file_in, 'preprocessed_out': file_out_dir}\n",
    "\n",
    "for root, dirs, files in os.walk(grab_amass_dir):\n",
    "    subject_id = os.path.basename(root)\n",
    "    if subject_id in data_dict:\n",
    "        for file in files:\n",
    "            # all processed files should have 'stageii' in their name\n",
    "            if 'stageii' not in file:\n",
    "                continue\n",
    "\n",
    "            task_description = os.path.splitext(file)[0].replace('_stageii', '')\n",
    "            if task_description in data_dict[subject_id]:\n",
    "                data_dict[subject_id][task_description]['amass_info'] = os.path.join(root, file)\n",
    "            elif 'pick_all' in task_description:\n",
    "                task_description = task_description.replace('pick_all', 'lift')\n",
    "                if task_description in data_dict[subject_id]:\n",
    "                    data_dict[subject_id][task_description]['amass_info'] = os.path.join(root, file)\n",
    "                else:\n",
    "                    print(f'No corresponding task for {task_description} in {subject_id} found')\n",
    "            else:\n",
    "                print(f'No corresponding task for {task_description} in {subject_id} found')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sbj_verts(seq_data):\n",
    "        mesh_path = os.path.join(grab_dir, '..',seq_data.body.vtemp)\n",
    "        sbj_vtemp = np.array(Mesh(filename=mesh_path).vertices)\n",
    "        return sbj_vtemp\n",
    "\n",
    "def load_obj_verts(obj_name, seq_data, n_verts_sample=512):\n",
    "    mesh_path = os.path.join(grab_dir, '..',seq_data.object.object_mesh)\n",
    "    np.random.seed(100)\n",
    "    obj_mesh = Mesh(filename=mesh_path)\n",
    "    verts_obj = np.array(obj_mesh.vertices)\n",
    "    faces_obj = np.array(obj_mesh.faces)\n",
    "\n",
    "    if verts_obj.shape[0] > n_verts_sample:\n",
    "        verts_sample_id = np.random.choice(verts_obj.shape[0], n_verts_sample, replace=False)\n",
    "    else:\n",
    "        verts_sample_id = np.arange(verts_obj.shape[0])\n",
    "\n",
    "    verts_sampled = verts_obj[verts_sample_id]\n",
    "    obj_info = {'verts': verts_obj,\n",
    "                'faces': faces_obj,\n",
    "                'verts_sample_id': verts_sample_id,\n",
    "                'verts_sample': verts_sampled,\n",
    "                'obj_mesh_file': mesh_path}\n",
    "\n",
    "    return obj_info\n",
    "\n",
    "save_body_verts = True\n",
    "save_lhand_verts = True\n",
    "save_rhand_verts = True\n",
    "save_object_verts = True\n",
    "save_contact = True\n",
    "n_verts_sample = 1000\n",
    "\n",
    "\n",
    "def process_data_entry(in_file, amass_file, out_dir):\n",
    "    body_data = {\n",
    "        'global_orient': [],'body_pose': [],'transl': [],\n",
    "        'right_hand_pose': [],'left_hand_pose': [],\n",
    "        'jaw_pose': [],'leye_pose': [],'reye_pose': [],\n",
    "        'expression': [],'fullpose': [],\n",
    "        'contact':[], 'verts' :[]\n",
    "    }\n",
    "\n",
    "    object_data = {'verts': [], 'global_orient': [], 'transl': [], 'contact': []}\n",
    "    lhand_data = {'verts': [], 'global_orient': [], 'hand_pose': [], 'transl': [], 'fullpose': []}\n",
    "    rhand_data = {'verts': [], 'global_orient': [], 'hand_pose': [], 'transl': [], 'fullpose': []}\n",
    "\n",
    "    seq_data = parse_npz(in_file)\n",
    "    amass_data = np.load(amass_file, allow_pickle=True)\n",
    "    smplh_data = {\n",
    "         'poses': amass_data['poses'], # T x 156\n",
    "         'betas': amass_data['betas'], # 16 -> need to reshape to T x 10\n",
    "         'trans': amass_data['trans'], # T x 3\n",
    "        #  'root_orient': amass_data['root_orient'], # T x 3, don't need this for BEHAVE format\n",
    "    }\n",
    "\n",
    "    # reshape betas to T x 10 with np.tile\n",
    "    smplh_data['betas'] = np.tile(smplh_data['betas'][:10], (smplh_data['poses'].shape[0], 1))\n",
    "    num_amass_timesteps = smplh_data['poses'].shape[0]\n",
    "    \n",
    "    obj_name = seq_data.obj_name\n",
    "    sbj_id   = seq_data.sbj_id\n",
    "    n_comps  = seq_data.n_comps\n",
    "    gender   = seq_data.gender\n",
    "\n",
    "    # need this for other methods from GRAB, this should not filter out any frames\n",
    "    frame_mask = (seq_data['contact']['object']>-1).any(axis=1)\n",
    "    T = frame_mask.sum()\n",
    "    \n",
    "    # make sure AMASS data has the same number of timesteps as GRAB data, otherwise something is wrong\n",
    "    assert num_amass_timesteps == T, f'Number of timesteps in GRAB and AMASS data do not match: {num_amass_timesteps} vs {T}'\n",
    "\n",
    "    sbj_params = prepare_params(seq_data.body.params, frame_mask)\n",
    "    rh_params  = prepare_params(seq_data.rhand.params, frame_mask)\n",
    "    lh_params  = prepare_params(seq_data.lhand.params, frame_mask)\n",
    "    obj_params = prepare_params(seq_data.object.params, frame_mask)\n",
    "\n",
    "    append2dict(body_data, sbj_params)\n",
    "    append2dict(rhand_data, rh_params)\n",
    "    append2dict(lhand_data, lh_params)\n",
    "    append2dict(object_data, obj_params)\n",
    "\n",
    "    sbj_vtemp = load_sbj_verts(seq_data)\n",
    "\n",
    "    if save_body_verts:\n",
    "\n",
    "        sbj_m = smplx.create(model_path=model_path,\n",
    "                                model_type='smplx',\n",
    "                                gender=gender,\n",
    "                                num_pca_comps=n_comps,\n",
    "                                v_template=sbj_vtemp,\n",
    "                                batch_size=T)\n",
    "\n",
    "        sbj_parms = params2torch(sbj_params)\n",
    "        verts_sbj = to_cpu(sbj_m(**sbj_parms).vertices)\n",
    "        body_data['verts'].append(verts_sbj)\n",
    "\n",
    "    if save_lhand_verts:\n",
    "        lh_mesh = os.path.join(grab_dir, '..', seq_data.lhand.vtemp)\n",
    "        lh_vtemp = np.array(Mesh(filename=lh_mesh).vertices)\n",
    "\n",
    "        lh_m = smplx.create(model_path=model_path,\n",
    "                            model_type='mano',\n",
    "                            is_rhand=False,\n",
    "                            v_template=lh_vtemp,\n",
    "                            num_pca_comps=n_comps,\n",
    "                            flat_hand_mean=True,\n",
    "                            batch_size=T)\n",
    "\n",
    "        lh_parms = params2torch(lh_params)\n",
    "        verts_lh = to_cpu(lh_m(**lh_parms).vertices)\n",
    "        lhand_data['verts'].append(verts_lh)\n",
    "\n",
    "    if save_rhand_verts:\n",
    "        rh_mesh = os.path.join(grab_dir, '..', seq_data.rhand.vtemp)\n",
    "        rh_vtemp = np.array(Mesh(filename=rh_mesh).vertices)\n",
    "\n",
    "        rh_m = smplx.create(model_path=model_path,\n",
    "                            model_type='mano',\n",
    "                            is_rhand=True,\n",
    "                            v_template=rh_vtemp,\n",
    "                            num_pca_comps=n_comps,\n",
    "                            flat_hand_mean=True,\n",
    "                            batch_size=T)\n",
    "\n",
    "        rh_parms = params2torch(rh_params)\n",
    "        verts_rh = to_cpu(rh_m(**rh_parms).vertices)\n",
    "        rhand_data['verts'].append(verts_rh)\n",
    "\n",
    "    ### for objects\n",
    "\n",
    "    obj_info = load_obj_verts(obj_name, seq_data, n_verts_sample)\n",
    "\n",
    "    if save_object_verts:\n",
    "\n",
    "        obj_m = ObjectModel(v_template=obj_info['verts_sample'],\n",
    "                            batch_size=T)\n",
    "        obj_parms = params2torch(obj_params)\n",
    "        verts_obj = to_cpu(obj_m(**obj_parms).vertices)\n",
    "        object_data['verts'].append(verts_obj)\n",
    "\n",
    "    if save_contact:\n",
    "        body_data['contact'].append(seq_data.contact.body[frame_mask])\n",
    "        object_data['contact'].append(seq_data.contact.object[frame_mask][:,obj_info['verts_sample_id']])\n",
    "\n",
    "    \n",
    "    behave_format_object_data = {\n",
    "        'angles': object_data['global_orient'][0], # T x 3\n",
    "        'trans': object_data['transl'][0] # T x 3\n",
    "    }\n",
    "\n",
    "    out_data = [body_data, lhand_data, rhand_data, object_data, smplh_data, behave_format_object_data]\n",
    "    out_data_name = ['body_data', 'rhand_data', 'lhand_data', 'object_data', 'smpl_fit_all', 'object_fit_all']\n",
    "    # save with numpy npz\n",
    "    for i in range(len(out_data)):\n",
    "        out_file = os.path.join(out_dir, f'{out_data_name[i]}.npz')\n",
    "        np.savez_compressed(out_file, **out_data[i])\n",
    "\n",
    "    print(f'Processed {in_file} and saved to {out_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing s5:   0%|          | 0/106 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a MANO model, with only 10 shape coefficients.\n",
      "WARNING: You are using a MANO model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing s5:   1%|          | 1/106 [00:06<10:45,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/data/grab_unzipped/grab/s5/flashlight_on_2.npz and saved to /media/erik/DATA/grab_preprocessed/s5_flashlight_on_2\n",
      "WARNING: You are using a MANO model, with only 10 shape coefficients.\n",
      "WARNING: You are using a MANO model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing s5:   1%|          | 1/106 [00:09<16:38,  9.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mprocess_data_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamass_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 157\u001b[0m, in \u001b[0;36mprocess_data_entry\u001b[0;34m(in_file, amass_file, out_dir)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out_data)):\n\u001b[1;32m    156\u001b[0m     out_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_data_name[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavez_compressed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msavez_compressed\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/grab/lib/python3.8/site-packages/numpy/lib/npyio.py:666\u001b[0m, in \u001b[0;36msavez_compressed\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_savez_compressed_dispatcher)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavez_compressed\u001b[39m(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    605\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m    Save several arrays into a single file in compressed ``.npz`` format.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[43m_savez\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/grab/lib/python3.8/site-packages/numpy/lib/npyio.py:699\u001b[0m, in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# always force zip64, gh-10776\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m zipf\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, force_zip64\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m--> 699\u001b[0m         \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m zipf\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/mambaforge/envs/grab/lib/python3.8/site-packages/numpy/lib/format.py:694\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[1;32m    692\u001b[0m             array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    693\u001b[0m             buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 694\u001b[0m         \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/grab/lib/python3.8/zipfile.py:1139\u001b[0m, in \u001b[0;36m_ZipWriteFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc \u001b[38;5;241m=\u001b[39m crc32(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc)\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compressor:\n\u001b[0;32m-> 1139\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# iterate over all files and process them, show tqdm progress bar\n",
    "for subject_id, task_description in data_dict.items():\n",
    "    for task_dict in tqdm(task_description.values()):\n",
    "        try:\n",
    "            in_file = task_dict['full_info']\n",
    "            amass_file = task_dict['amass_info']\n",
    "            out_dir = task_dict['preprocessed_out']\n",
    "        except KeyError:\n",
    "            print(f'No file found for {subject_id} - {task_description}')\n",
    "            continue\n",
    "        process_data_entry(in_file, amass_file, out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dexhoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
