{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import os, glob\n",
    "import smplx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GRAB.tools.objectmodel import ObjectModel\n",
    "from GRAB.tools.utils import parse_npz, prepare_params, params2torch, to_cpu, append2dict\n",
    "from GRAB.tools.meshviewer import Mesh\n",
    "import trimesh\n",
    "\n",
    "from manopth.manolayer import ManoLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data directory: /media/erik/DATA/grab/grab_extracted/grab\n",
      "AMASS directory: /media/erik/DATA/grab/grab_amass\n",
      "Output directory: /media/erik/DATA/grab/grab_preprocessed\n",
      "SMPLX/MANO Model directory: /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/body_models\n",
      "Text annotation file: /home/erik/ethz/digital-humans/dex-hoi/data_preprocessing/grab_preprocessing/grab_annotations.csv\n",
      "Text annotation output directory: /media/erik/DATA/texts\n",
      "Object mesh directory: /media/erik/DATA/grab/grab_extracted/tools/object_meshes/contact_meshes\n",
      "Object mesh output directory: /media/erik/DATA/object_mesh\n",
      "Object sample output directory: /media/erik/DATA/object_sample\n",
      "Hand output directory: /media/erik/DATA/hands\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data input dirs\n",
    "root_dir = '/media/erik/DATA/grab'\n",
    "grab_dir = os.path.abspath(f'{root_dir}/grab_extracted/grab')\n",
    "grab_amass_dir = os.path.abspath(f'{root_dir}/grab_amass')\n",
    "model_path = os.path.abspath('./body_models/')\n",
    "\n",
    "# out dir for reorganized SMPL and object data for the HumanML3D pipeline\n",
    "out_dir = os.path.abspath(f'{root_dir}/grab_preprocessed')\n",
    "\n",
    "# texts\n",
    "text_annotation_path = os.path.abspath('./grab_annotations.csv')\n",
    "text_annotation_out_dir = os.path.abspath('/media/erik/DATA/texts/')\n",
    "\n",
    "# objects\n",
    "object_mesh_dir = os.path.abspath(f'{root_dir}/grab_extracted/tools/object_meshes/contact_meshes')\n",
    "object_mesh_out_dir = os.path.abspath('/media/erik/DATA/object_mesh')\n",
    "object_sample_out_dir = os.path.abspath('/media/erik/DATA/object_sample')\n",
    "\n",
    "# hand output directory\n",
    "hand_out_dir = os.path.abspath('/media/erik/DATA/hands')\n",
    "\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "os.makedirs(text_annotation_out_dir, exist_ok=True)\n",
    "os.makedirs(object_mesh_out_dir, exist_ok=True)\n",
    "os.makedirs(object_sample_out_dir, exist_ok=True)\n",
    "os.makedirs(hand_out_dir, exist_ok=True)\n",
    "\n",
    "print('Using device:', device)\n",
    "print('Data directory:', grab_dir)\n",
    "print('AMASS directory:', grab_amass_dir)\n",
    "print('Output directory:', out_dir)\n",
    "print('SMPLX/MANO Model directory:', model_path)\n",
    "print('Text annotation file:', text_annotation_path)\n",
    "print('Text annotation output directory:', text_annotation_out_dir)\n",
    "print('Object mesh directory:', object_mesh_dir)\n",
    "print('Object mesh output directory:', object_mesh_out_dir)\n",
    "print('Object sample output directory:', object_sample_out_dir)\n",
    "print('Hand output directory:', hand_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s2 with 93 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s6 with 148 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s9 with 125 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s1 with 198 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s10 with 145 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s5 with 106 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s8 with 162 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s3 with 125 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s4 with 113 files, creating output folder structure...\n",
      "Adding directory /media/erik/DATA/grab/grab_extracted/grab/s7 with 120 files, creating output folder structure...\n",
      "No corresponding task for banana_eat_1 in s5 found\n",
      "No corresponding task for phone_lift in s5 found\n",
      "No corresponding task for cylindersmall_lift in s5 found\n",
      "No corresponding task for airplane_fly_1 in s5 found\n",
      "No corresponding task for multi_bottle_whineglass_1 in s3 found\n"
     ]
    }
   ],
   "source": [
    "# map directory names to lists of files in them\n",
    "data_dict = {}\n",
    "\n",
    "for root, dirs, files in os.walk(grab_dir):\n",
    "    if len(files) > 0:\n",
    "        print(f'Adding directory {root} with {len(files)} files, creating output folder structure...')\n",
    "        subject_id = os.path.basename(root)\n",
    "        data_dict[subject_id] = {}\n",
    "\n",
    "        for file in files:\n",
    "            task_description = os.path.splitext(file)[0]\n",
    "            file_in = os.path.join(root, file)\n",
    "            file_out_dir = os.path.join(out_dir, f'{subject_id}_{task_description}')\n",
    "            os.makedirs(file_out_dir, exist_ok=True)\n",
    "            data_dict[subject_id][task_description] = {'full_info': file_in, 'preprocessed_out': file_out_dir}\n",
    "\n",
    "for root, dirs, files in os.walk(grab_amass_dir):\n",
    "    subject_id = os.path.basename(root)\n",
    "    if subject_id in data_dict:\n",
    "        for file in files:\n",
    "            # all processed files should have 'stageii' in their name\n",
    "            if 'stageii' not in file:\n",
    "                continue\n",
    "\n",
    "            task_description = os.path.splitext(file)[0].replace('_stageii', '')\n",
    "            if task_description in data_dict[subject_id]:\n",
    "                data_dict[subject_id][task_description]['amass_info'] = os.path.join(root, file)\n",
    "            elif 'pick_all' in task_description:\n",
    "                task_description = task_description.replace('pick_all', 'lift')\n",
    "                if task_description in data_dict[subject_id]:\n",
    "                    data_dict[subject_id][task_description]['amass_info'] = os.path.join(root, file)\n",
    "                else:\n",
    "                    print(f'No corresponding task for {task_description} in {subject_id} found')\n",
    "            else:\n",
    "                print(f'No corresponding task for {task_description} in {subject_id} found')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all object mesh files (.ply)\n",
    "object_mesh_files = glob.glob(os.path.join(object_mesh_dir, '*.ply'))\n",
    "\n",
    "# create a folder inside object_mesh_out_dir for each object and copy the mesh file there\n",
    "for obj_mesh_file in object_mesh_files:\n",
    "    obj_name = os.path.basename(obj_mesh_file).replace('.ply', '')\n",
    "    obj_out_dir = os.path.join(object_mesh_out_dir, obj_name)\n",
    "    os.makedirs(obj_out_dir, exist_ok=True)\n",
    "    os.system(f'cp {obj_mesh_file} {obj_out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1333/1333 [00:06<00:00, 219.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from text_preprocessing import process_grab\n",
    "import pandas as pd\n",
    "\n",
    "# load csv dataset as dataframe\n",
    "df = pd.read_csv(text_annotation_path, header=None, names=['seq_name', 'caption'])\n",
    "os.makedirs(text_annotation_out_dir, exist_ok=True)\n",
    "\n",
    "process_grab(df, out_path=text_annotation_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sbj_verts(seq_data):\n",
    "        mesh_path = os.path.join(grab_dir, '..',seq_data.body.vtemp)\n",
    "        sbj_vtemp = np.array(Mesh(filename=mesh_path).vertices)\n",
    "        return sbj_vtemp\n",
    "\n",
    "def load_obj_verts(obj_name, seq_data, n_verts_sample=512):\n",
    "    mesh_path = os.path.join(grab_dir, '..',seq_data.object.object_mesh)\n",
    "    np.random.seed(100)\n",
    "    obj_mesh = Mesh(filename=mesh_path)\n",
    "    verts_obj = np.array(obj_mesh.vertices)\n",
    "    faces_obj = np.array(obj_mesh.faces)\n",
    "\n",
    "    if verts_obj.shape[0] > n_verts_sample:\n",
    "        verts_sample_id = np.random.choice(verts_obj.shape[0], n_verts_sample, replace=False)\n",
    "    else:\n",
    "        verts_sample_id = np.arange(verts_obj.shape[0])\n",
    "\n",
    "    verts_sampled = verts_obj[verts_sample_id]\n",
    "    obj_info = {'verts': verts_obj,\n",
    "                'faces': faces_obj,\n",
    "                'verts_sample_id': verts_sample_id,\n",
    "                'verts_sample': verts_sampled,\n",
    "                'obj_mesh_file': mesh_path}\n",
    "\n",
    "    return obj_info\n",
    "\n",
    "save_body_verts = False\n",
    "save_lhand_verts = True\n",
    "save_rhand_verts = True\n",
    "save_object_verts = True\n",
    "save_contact = True\n",
    "n_verts_sample = 512\n",
    "n_comps_hands = 24\n",
    "\n",
    "\n",
    "def process_data_entry(in_file, amass_file, out_dir):\n",
    "    body_data = {\n",
    "        'global_orient': [],'body_pose': [],'transl': [],\n",
    "        'right_hand_pose': [],'left_hand_pose': [],\n",
    "        'jaw_pose': [],'leye_pose': [],'reye_pose': [],\n",
    "        'expression': [],'fullpose': [],\n",
    "        'contact':[], 'verts' :[]\n",
    "    }\n",
    "\n",
    "    object_data = {'verts': [], 'global_orient': [], 'transl': [], 'contact': []}\n",
    "    lhand_data = {'verts': [], 'global_orient': [], 'hand_pose': [], 'transl': [], 'fullpose': [], 'joints': []}\n",
    "    rhand_data = {'verts': [], 'global_orient': [], 'hand_pose': [], 'transl': [], 'fullpose': [], 'joints': []}\n",
    "    print(f'Processing {in_file} and {amass_file}')\n",
    "    seq_data = parse_npz(in_file)\n",
    "    amass_data = np.load(amass_file, allow_pickle=True)\n",
    "    smplh_data = {\n",
    "         'poses': amass_data['poses'], # T x 156\n",
    "         'betas': amass_data['betas'], # 16 -> need to reshape to T x 10\n",
    "         'trans': amass_data['trans'], # T x 3\n",
    "        #  'root_orient': amass_data['root_orient'], # T x 3, don't need this for BEHAVE format\n",
    "    }\n",
    "\n",
    "    # reshape betas to T x 10 with np.tile\n",
    "    smplh_data['betas'] = np.tile(smplh_data['betas'][:10], (smplh_data['poses'].shape[0], 1))\n",
    "    num_amass_timesteps = smplh_data['poses'].shape[0]\n",
    "    \n",
    "    obj_name = seq_data.obj_name\n",
    "    sbj_id   = seq_data.sbj_id\n",
    "    n_comps  = seq_data.n_comps\n",
    "    gender   = seq_data.gender\n",
    "\n",
    "    # need this for other methods from GRAB, this should not filter out any frames\n",
    "    frame_mask = (seq_data['contact']['object']>-1).any(axis=1)\n",
    "    T = frame_mask.sum()\n",
    "    \n",
    "    # make sure AMASS data has the same number of timesteps as GRAB data, otherwise something is wrong\n",
    "    assert num_amass_timesteps == T, f'Number of timesteps in GRAB and AMASS data do not match: {num_amass_timesteps} vs {T}'\n",
    "\n",
    "    sbj_params = prepare_params(seq_data.body.params, frame_mask)\n",
    "    rh_params  = prepare_params(seq_data.rhand.params, frame_mask)\n",
    "    lh_params  = prepare_params(seq_data.lhand.params, frame_mask)\n",
    "    obj_params = prepare_params(seq_data.object.params, frame_mask)\n",
    "\n",
    "    print(f'lhand params: {lh_params.keys()}')\n",
    "    append2dict(body_data, sbj_params)\n",
    "    append2dict(rhand_data, rh_params)\n",
    "    append2dict(lhand_data, lh_params)\n",
    "    append2dict(object_data, obj_params)\n",
    "\n",
    "    sbj_vtemp = load_sbj_verts(seq_data)\n",
    "\n",
    "    if save_body_verts:\n",
    "\n",
    "        sbj_m = smplx.create(model_path=model_path,\n",
    "                                model_type='smplx',\n",
    "                                gender=gender,\n",
    "                                num_pca_comps=n_comps,\n",
    "                                v_template=sbj_vtemp,\n",
    "                                batch_size=T)\n",
    "\n",
    "        sbj_parms = params2torch(sbj_params)\n",
    "        verts_sbj = to_cpu(sbj_m(**sbj_parms).vertices)\n",
    "        body_data['verts'].append(verts_sbj)\n",
    "\n",
    "    if save_lhand_verts:\n",
    "        lh_mesh = os.path.join(grab_dir, '..', seq_data.lhand.vtemp)\n",
    "        lh_vtemp = np.array(Mesh(filename=lh_mesh).vertices)\n",
    "\n",
    "        lh_m = smplx.create(model_path=model_path,\n",
    "                            model_type='mano',\n",
    "                            is_rhand=False,\n",
    "                            v_template=lh_vtemp,\n",
    "                            num_pca_comps=n_comps_hands,\n",
    "                            flat_hand_mean=True,\n",
    "                            batch_size=T)\n",
    "\n",
    "        print(f'lh params: {lh_params[\"hand_pose\"].shape}')\n",
    "        lh_parms = params2torch(lh_params)\n",
    "        model_output = lh_m(**lh_parms)\n",
    "        # verts_lh = to_cpu(model_output.vertices)\n",
    "\n",
    "        hand_pose = model_output.hand_pose # T x 45-dim\n",
    "        betas = model_output.betas # T x 10-dim\n",
    "\n",
    "        # only use n_comps_hands PCA components\n",
    "        lhand_data['pca_pose'] = to_cpu(hand_pose)\n",
    "\n",
    "        # lhand_data['verts'].append(verts_lh)\n",
    "\n",
    "    if save_rhand_verts:\n",
    "        rh_mesh = os.path.join(grab_dir, '..', seq_data.rhand.vtemp)\n",
    "        rh_vtemp = np.array(Mesh(filename=rh_mesh).vertices)\n",
    "\n",
    "        rh_m = smplx.create(model_path=model_path,\n",
    "                            model_type='mano',\n",
    "                            is_rhand=True,\n",
    "                            v_template=rh_vtemp,\n",
    "                            num_pca_comps=n_comps_hands,\n",
    "                            flat_hand_mean=True,\n",
    "                            batch_size=T)\n",
    "\n",
    "        rh_parms = params2torch(rh_params)\n",
    "        model_output = rh_m(**rh_parms)\n",
    "        # verts_rh = to_cpu(model_output.vertices)\n",
    "        \n",
    "        hand_pose = model_output.hand_pose # T x 45-dim\n",
    "        betas = model_output.betas # T x 10-dim\n",
    "\n",
    "        # only use n_comps_hands PCA components\n",
    "        rhand_data['pca_pose'] = to_cpu(hand_pose)\n",
    "\n",
    "        # print(f'HAND DATA PCA SHAPE: {rhand_data[\"pca_pose\"].shape}')\n",
    "        # rhand_data['verts'].append(verts_rh)\n",
    "\n",
    "    ### for objects\n",
    "\n",
    "    obj_info = load_obj_verts(obj_name, seq_data, n_verts_sample)\n",
    "\n",
    "    if save_object_verts:\n",
    "\n",
    "        obj_m = ObjectModel(v_template=obj_info['verts_sample'],\n",
    "                            batch_size=T)\n",
    "        obj_parms = params2torch(obj_params)\n",
    "        verts_obj = to_cpu(obj_m(**obj_parms).vertices)\n",
    "        object_data['verts'].append(verts_obj)\n",
    "\n",
    "    if save_contact:\n",
    "        body_data['contact'].append(seq_data.contact.body[frame_mask])\n",
    "        object_data['contact'].append(seq_data.contact.object[frame_mask][:,obj_info['verts_sample_id']])\n",
    "\n",
    "    \n",
    "    behave_format_object_data = {\n",
    "        'angles': object_data['global_orient'][0], # T x 3\n",
    "        'trans': object_data['transl'][0] # T x 3\n",
    "    }\n",
    "\n",
    "    # save left hand, right hand data in hand_out_dir\n",
    "    # lhand_out_file = os.path.join(hand_out_dir, f'{os.path.basename(out_dir)}_lhand.npy')\n",
    "    # rhand_out_file = os.path.join(hand_out_dir, f'{os.path.basename(out_dir)}_rhand.npy')\n",
    "    # np.save(lhand_out_file, lhand_data)\n",
    "    # np.save(rhand_out_file, rhand_data)\n",
    "\n",
    "    out_data = [body_data, rhand_data, lhand_data, object_data, smplh_data, behave_format_object_data]\n",
    "    out_data_name = ['body_data', 'rhand_data', 'lhand_data','object_data', 'smpl_fit_all', 'object_fit_all']\n",
    "    # save with numpy npz\n",
    "    for i in range(len(out_data)):\n",
    "        out_file = os.path.join(out_dir, f'{out_data_name[i]}.npz')\n",
    "        np.savez_compressed(out_file, **out_data[i])\n",
    "\n",
    "    info_dict = {\n",
    "        'gender': gender,\n",
    "    }\n",
    "    json_str = json.dumps(info_dict)\n",
    "    with open(os.path.join(out_dir, 'info.json'), 'w') as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "    # save object sample ids in object_sample_out_dir\n",
    "    obj_sample_out_file = os.path.join(object_sample_out_dir, f'{os.path.basename(out_dir)}.npy')\n",
    "    np.save(obj_sample_out_file, obj_info['verts_sample_id'])\n",
    "\n",
    "    print(f'Processed {in_file}, saved motion info to {out_dir} and object sample to {obj_sample_out_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edef0588e2f64f58b3f87fc7c4a712d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /media/erik/DATA/grab/grab_extracted/grab/s2/flashlight_on_1.npz and /media/erik/DATA/grab/grab_amass/GRAB/s2/flashlight_on_1_stageii.npz\n",
      "lhand params: dict_keys(['global_orient', 'hand_pose', 'transl', 'fullpose'])\n",
      "WARNING: You are using a MANO model, with only 10 shape coefficients.\n",
      "lh params: (1041, 24)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (27) must match the size of tensor b (48) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mprocess_data_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamass_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_out_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 114\u001b[0m, in \u001b[0;36mprocess_data_entry\u001b[0;34m(in_file, amass_file, out_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlh params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlh_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhand_pose\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m lh_parms \u001b[38;5;241m=\u001b[39m params2torch(lh_params)\n\u001b[0;32m--> 114\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mlh_m\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlh_parms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# verts_lh = to_cpu(model_output.vertices)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m hand_pose \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mhand_pose \u001b[38;5;66;03m# T x 45-dim\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/grab_env/lib/python3.8/site-packages/smplx/body_models.py:1672\u001b[0m, in \u001b[0;36mMANO.forward\u001b[0;34m(self, betas, global_orient, hand_pose, transl, return_verts, return_full_pose, **kwargs)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     hand_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m   1669\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi,ij->bj\u001b[39m\u001b[38;5;124m'\u001b[39m, [hand_pose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhand_components])\n\u001b[1;32m   1671\u001b[0m full_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([global_orient, hand_pose], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1672\u001b[0m full_pose \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_mean\n\u001b[1;32m   1674\u001b[0m vertices, joints \u001b[38;5;241m=\u001b[39m lbs(betas, full_pose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_template,\n\u001b[1;32m   1675\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshapedirs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposedirs,\n\u001b[1;32m   1676\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_regressor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparents,\n\u001b[1;32m   1677\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlbs_weights, pose2rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1678\u001b[0m                        )\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# # Add pre-selected extra joints that might be needed\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# joints = self.vertex_joint_selector(vertices, joints)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (27) must match the size of tensor b (48) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# iterate over all files and process them, show tqdm progress bar\n",
    "for subject_id, task_description in data_dict.items():\n",
    "    for task_dict in tqdm(task_description.values()):\n",
    "        try:\n",
    "            in_file = task_dict['full_info']\n",
    "            amass_file = task_dict['amass_info']\n",
    "            task_out_dir = task_dict['preprocessed_out']\n",
    "        except KeyError:\n",
    "            print(f'No file found for {subject_id} - {task_description}')\n",
    "            continue\n",
    "        process_data_entry(in_file, amass_file, task_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving split file to /media/erik/DATA/grab/grab_preprocessed/../split.json\n",
      "Saved train and test files to /media/erik/DATA/grab/grab_preprocessed/../train.txt and /media/erik/DATA/grab/grab_preprocessed/../test.txt\n"
     ]
    }
   ],
   "source": [
    "# generate a file called split.json in parent directory of out_dir, which has a list for training and test splits that contain sequence names\n",
    "\n",
    "# get all sequence names - these are subdirectories in out_dir\n",
    "seq_names = [os.path.basename(x) for x in glob.glob(os.path.join(out_dir, '*'))]\n",
    "# print(seq_names[:10])\n",
    "train_test_ratio = 0.8\n",
    "# randomly select train_test_ratio of the sequences for training\n",
    "np.random.seed(100)\n",
    "np.random.shuffle(seq_names)\n",
    "split_idx = int(len(seq_names) * train_test_ratio)\n",
    "train_seq_names = seq_names[:split_idx]\n",
    "test_seq_names = seq_names[split_idx:]\n",
    "\n",
    "split_dict = {\n",
    "    'train': train_seq_names,\n",
    "    'test': test_seq_names\n",
    "}\n",
    "\n",
    "split_file = os.path.join(out_dir, '..', 'split.json')\n",
    "json_str = json.dumps(split_dict, indent=4)\n",
    "\n",
    "print(f'Saving split file to {split_file}')\n",
    "with open(split_file, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "# also save 2 test files called train.txt and test.txt in the same directory, each line containing a direcotry name\n",
    "train_file = os.path.join(out_dir, '..', 'train.txt')\n",
    "test_file = os.path.join(out_dir, '..', 'test.txt')\n",
    "\n",
    "with open(train_file, 'w') as f:\n",
    "    for seq_name in train_seq_names:\n",
    "        f.write(f'{seq_name}\\n')\n",
    "\n",
    "with open(test_file, 'w') as f:\n",
    "    for seq_name in test_seq_names:\n",
    "        f.write(f'{seq_name}\\n')\n",
    "\n",
    "print(f'Saved train and test files to {train_file} and {test_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dexhoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
